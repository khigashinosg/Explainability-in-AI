# Explainability in AI

## Project Description
This project delves into the field of explainable AI, specifically focusing on feature attribution methods and counterfactual explanations. It utilizes various approaches like SHAP, DeepLift, and counterfactual techniques to analyze models' decision-making processes. The goal is to make AI decision-making transparent and understandable, enhancing the trustworthiness of AI systems.

## Installation
pip install -r requirements.txt

Ensure you are using Python 3.10.

## Usage
- `exploratory_analysis.py`: Perform exploratory data analysis on your dataset.
- `feature_attribution.py`: Run feature attribution methods like SHAP and DeepLift.
- `counterfactuals.py`: Generate and analyze counterfactual explanations.

## Contributing
Contributions are welcome! Please open an issue to discuss proposed changes or open a pull request with your contributions.

## Authors
- [Your Name]
- [Other Contributors]

## License
This project is licensed under the MIT License - see the LICENSE file for details.

## Project Status
This project is currently in the maintenance phase. New contributions and bug fixes are welcome, but active development has slowed down.
