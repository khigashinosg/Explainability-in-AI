{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4907a9b-7f61-4e03-a118-3ca33156b516",
   "metadata": {},
   "source": [
    "# Ethics, Fairness and Explanation in AI Coursework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18977034-9183-47c7-993e-9b105ebaf8bc",
   "metadata": {},
   "source": [
    "Your goal in this coursework is to implement and experiment with various explainability approaches in order to better understand the behaviour of a neural model applied to the Titanic dataset. As you will have a chance to observe, the dataset reflects some of the past social conventions and biases, which also affect the trained model. Explanations can serve as very useful tools for identifying such potential issues and gaining insight into the internal reasoning of machine learning systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54e2fe-cc8f-4b0d-852a-474e43b5b060",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f683e86-6c7d-4ce1-9cc9-a3ec171824d8",
   "metadata": {},
   "source": [
    "We start by defining some helpful utility functions for data preprocessing. You will probably not need to change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3853c0d-4ab5-41dc-a409-ef3b7dab0911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "class InvertibleColumnTransformer(ColumnTransformer):\n",
    "    \"\"\"\n",
    "    This is an invertible version of a ColumnTransformer from sklearn.\n",
    "    This allows us to recover the original feature values from their normalised\n",
    "    versions in order to better understand the produced explanations.\n",
    "    \"\"\"\n",
    "    def inverse_transform(self, X):\n",
    "        if X.ndim == 1:\n",
    "            X = np.expand_dims(X, axis=0)\n",
    "        if X.shape[1] != len(self.get_feature_names_out()):\n",
    "            raise ValueError(\n",
    "                \"X and the fitted transformer have different numbers of columns\"\n",
    "            )\n",
    "\n",
    "        inverted_X_base = np.zeros((X.shape[0], self.n_features_in_))\n",
    "        columns = [c for cs in self._columns for c in cs]\n",
    "        inverted_X = pd.DataFrame(data=inverted_X_base, columns=columns)\n",
    "        inverted_X = inverted_X.astype('object')\n",
    "        for name, indices in self.output_indices_.items():\n",
    "            transformer = self.named_transformers_.get(name, None)\n",
    "            if transformer is None:\n",
    "                continue\n",
    "\n",
    "            selected_X = X[:, indices.start : indices.stop]\n",
    "            if isinstance(transformer, OneHotEncoder):\n",
    "                # Assumed only one column changing encoder at the end\n",
    "                categories = transformer.inverse_transform(selected_X)\n",
    "                inverted_X.loc[\n",
    "                    :, columns[indices.start : indices.start + len(categories[0])]\n",
    "                ] = categories\n",
    "            else:\n",
    "                # Assumed scaler-type transformer\n",
    "                inverted_X.loc[\n",
    "                    :, [columns[i] for i in range(indices.start, indices.stop)]\n",
    "                ] = transformer.inverse_transform(selected_X)\n",
    "\n",
    "        return inverted_X\n",
    "\n",
    "\n",
    "def preprocess_train_data(\n",
    "    df,\n",
    "    scaled_features=None,\n",
    "    categorical_features=None,\n",
    "    scaler=RobustScaler(quantile_range=(10, 90)),\n",
    "    categorical_encoder=OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    Scales the continuous features using a RobustScaler and one-hot encodes\n",
    "    the categorical features.\n",
    "    \"\"\"\n",
    "    if scaled_features is None and categorical_features is None:\n",
    "        warnings.warn(\"No features specified for preprocessing, using raw data.\")\n",
    "        scaled_features = []\n",
    "        categorical_features = []\n",
    "    elif scaled_features is None:\n",
    "        scaled_features = [c for c in df.columns if c not in categorical_features]\n",
    "    elif categorical_features is None:\n",
    "        categorical_features = [c for c in df.columns if c not in scaled_features]\n",
    "\n",
    "    preprocessor = InvertibleColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", scaler, scaled_features),\n",
    "            (\"cat\", categorical_encoder, categorical_features),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "\n",
    "    preprocessed_df = preprocessor.fit_transform(df)\n",
    "    return preprocessed_df, preprocessor\n",
    "\n",
    "\n",
    "def preprocess_test_data(df, preprocessor):\n",
    "    preprocessed_df = preprocessor.transform(df)\n",
    "    return preprocessed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df35c3a2-09c0-4a14-9f83-e81db1d544a1",
   "metadata": {},
   "source": [
    "Here, we define a class for the Titanic dataset, which we will be using throughout the coursework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7f44d-1cbe-4e25-9f7c-7bbe8c2d4cc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the Titanic dataset.\n",
    "    \"\"\"\n",
    "    __create_key = object()\n",
    "\n",
    "    @classmethod\n",
    "    def create_datasets(\n",
    "        cls,\n",
    "        label_name=\"survived\",\n",
    "        split_seed=42,\n",
    "        test_size=0.2,\n",
    "    ):\n",
    "        train_dataset = TitanicDataset(\n",
    "            cls.__create_key,\n",
    "            label_name=label_name,\n",
    "            split_seed=split_seed,\n",
    "            test_size=test_size,\n",
    "            train=True,\n",
    "        )\n",
    "        test_dataset = TitanicDataset(\n",
    "            cls.__create_key,\n",
    "            label_name=label_name,\n",
    "            split_seed=split_seed,\n",
    "            test_size=test_size,\n",
    "            train=False,\n",
    "        )\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        create_key=None,\n",
    "        label_name=\"Survived\",\n",
    "        split_seed=42,\n",
    "        test_size=0.2,\n",
    "        train=True,\n",
    "    ):\n",
    "        # Ensure that the dataset is being constructed properly\n",
    "        if create_key != TitanicDataset.__create_key:\n",
    "            raise ValueError(\n",
    "                \"Illegal initialisation attempt — please use create_datasets to initialise.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            data_df = pd.read_csv(\"titanic-dataset.csv\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"Titanic data file not found.\")\n",
    "\n",
    "        # Split the dataset into train and test\n",
    "        x = data_df.drop(columns=[label_name, \"name\", \"ticket\", \"cabin\", \"embarked\", \"boat\", \"body\", \"home.dest\"])\n",
    "        # For the purposes of this coursework, we just impute the missing age and fare with a median value\n",
    "        x[['age']] = x[['age']].fillna(x[['age']].median())\n",
    "        x[['fare']] = x[['fare']].fillna(x[['fare']].median())\n",
    "        y = data_df[label_name]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            x, y, test_size=test_size, random_state=split_seed, shuffle=True\n",
    "        )\n",
    "        if train:\n",
    "            self.raw_data = x_train, y_train\n",
    "        else:\n",
    "            self.raw_data = x_test, y_test\n",
    "\n",
    "        # Preprocess the data\n",
    "        x_train_processed, preprocessor = preprocess_train_data(\n",
    "            x_train, categorical_features=[\"sex\"]\n",
    "        )\n",
    "        x_train = pd.DataFrame(\n",
    "            x_train_processed, columns=preprocessor.get_feature_names_out()\n",
    "        )\n",
    "        x_test_processed = preprocess_test_data(x_test, preprocessor)\n",
    "        x_test = pd.DataFrame(\n",
    "            x_test_processed, columns=preprocessor.get_feature_names_out()\n",
    "        )\n",
    "\n",
    "        # Select data partition and convert to tensors\n",
    "        if train:\n",
    "            samples = x_train\n",
    "            labels = y_train\n",
    "        else:\n",
    "            samples = x_test\n",
    "            labels = y_test\n",
    "        self.samples = torch.tensor(samples.to_numpy(), dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.to_numpy(), dtype=torch.long)\n",
    "        self.features = preprocessor.get_feature_names_out()\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c7c1f-81f5-4806-ad20-358a79df93e6",
   "metadata": {},
   "source": [
    "Finally, we call the code above to load and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae2d1e0-ddab-419b-b741-6d4d6b531bb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = TitanicDataset.create_datasets(\n",
    "    test_size=0.2,\n",
    "    split_seed=42,\n",
    ")\n",
    "train_dl = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4283a2e-986b-4554-89e3-3a0c4c152cbb",
   "metadata": {},
   "source": [
    "Note that the invertible transformer allows you to recover the original (unnormalised) feature values, as shown on the example below. You may find this helpful for understanding the produced explanations and commenting on them in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f85249-8c4b-46b0-a856-76c972f524e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_dataset.preprocessor.inverse_transform(test_dataset.samples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441756a7-f828-4764-af69-4eb84b63a975",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93538903-f8cc-4f39-9593-f0ee82096b75",
   "metadata": {},
   "source": [
    "When faced with a new dataset, it is a good practice to perform an exploratory data analysis in order to understand the basic trends in the data. This will also allow you to put the explanations you obtain as part of this coursework into the relevant context. We will use the raw, unnormalised features for this purpose, as they are much more intuitive and human-understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a5efb-2ce7-4428-a03f-ee0f61f6dca0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "x_train, y_train = train_dataset.raw_data\n",
    "x_train['survived'] = y_train\n",
    "data_df = x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf759d8-e225-4482-a8ba-93d0a604d32e",
   "metadata": {},
   "source": [
    "We start by displaying the feature values and labels for a few samples. The dataset contains data regarding the survival of some of the passengers involved in the [Titanic maritime disaster](https://en.wikipedia.org/wiki/Sinking_of_the_Titanic). The features contained in the data are as follows:\n",
    "* `pclass`: Indicates the travelling class of the given passenger. Note that we treat this feature as numerical, as the different classes introduce a natural order.\n",
    "* `sex`: Indicates the sex of the passenger.\n",
    "* `age`: Provides the age of the passenger.\n",
    "* `sibsp`: Denotes the total number of siblings and spouses of the given passenger also travelling on RMS Titanic.\n",
    "* `parch`: Denotes the total number of parents or children of the given passenger also travelling on RMS Titanic.\n",
    "* `fare`: Indicates the fare paid by the passenger for the journey.\n",
    "* `survived`: The label indicating whether the patient survived the accident (1 = survived, 0 = did not survive).\n",
    "\n",
    "There are other features included in the original dataset (such aspassenger name or point of embarkation), but we choose to ignore them for the purposes of this coursework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc9b42a-9f57-4502-8d00-9fda6bdb34cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d8e9c2-2266-40c2-9d91-3bd25e633eb2",
   "metadata": {},
   "source": [
    "Let us visualise the correlation between the individual columns of the data, computed using the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient). Note that we excluded the `sex` feature from this visualisation, as it is categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f7fb2-3f81-4f9a-9ff9-09f45d3a50e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "corr = data_df.drop(columns=[\"sex\"]).corr()\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    xticklabels=corr.columns.values,\n",
    "    yticklabels=corr.columns.values,\n",
    "    annot=True,\n",
    "    fmt='.2g',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba12652-00ee-42f2-b37c-7aabbd7d6b17",
   "metadata": {},
   "source": [
    "Since the previous plot does not include the categorical `sex` column, we also separately visualise its distribution, including the associated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e255d4-c9d4-4588-aeb3-f563c202b595",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data_df.groupby(['sex', 'survived']).size().unstack().plot(\n",
    "    kind='bar', stacked=True, color=['#a51900', '#02893b'], xlabel=\"Sex\", ylabel=\"Count\"\n",
    ")\n",
    "plt.legend(['No', 'Yes'], title=\"Survived\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2dc6b6-a871-4509-8049-52d84833c417",
   "metadata": {},
   "source": [
    "**Task 1(a)**: <br />\n",
    "**(i)** Considering the above visualisations, are there any trends or patterns that you can identify in the data? <br />\n",
    "**(ii)** Without having access to any particular model or the associated explanations, which features would you expect to be the most and least important for a neural network trained on the dataset? How can you tell and how certain can you be of your assessment? <br />\n",
    "**(iii)** Apart from inspecting the above plots, is there anything else you could do as part of the exploratory analysis that would allow you to better understand the data and the behaviour of the models trained on it? <br />\n",
    "Please write your answers in a few sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a7e4a2-988d-4580-9fb9-a93f76c3164b",
   "metadata": {},
   "source": [
    "## Model Initialisation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435873c-e279-45c9-b2b0-38faedba32ac",
   "metadata": {},
   "source": [
    "First, we define a global device variable to enable running this code on a GPU or a CPU, as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a56bef-3daa-44aa-8e57-d001b3bff2ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a96541-7047-4c2b-845f-b823f4a8b239",
   "metadata": {},
   "source": [
    "Here, we define several utility functions for constructing, training and evaluating neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395dd993-e2e1-46a8-b727-9f646ce4e409",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torcheval.metrics.functional import binary_f1_score, binary_accuracy, binary_auroc\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def construct_nn(nn_dims, activation_fun):\n",
    "    \"\"\"\n",
    "    Constructs a neural network with the specified architecture.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for i in range(1, len(nn_dims)):\n",
    "        in_dim, out_dim = nn_dims[i-1], nn_dims[i]\n",
    "        layers.append(nn.Linear(in_dim, out_dim))\n",
    "        layers.append(activation_fun())\n",
    "    # Remove the last activation layer and add Sigmoid instead\n",
    "    layers = layers[:-1]\n",
    "    layers.append(nn.Sigmoid())\n",
    "    \n",
    "    return nn.Sequential(*layers).to(DEVICE)\n",
    "\n",
    "def train_nn(model, train_dl, num_epochs=100):\n",
    "    \"\"\"\n",
    "    Trains a neural network using the data from the provided data loader.\n",
    "    \"\"\"\n",
    "    loss_fun = nn.BCELoss()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=0.005)\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in tqdm(range(num_epochs), leave=False):\n",
    "        total_loss = 0\n",
    "        for i, (x, y) in list(enumerate(train_dl)):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = loss_fun(out.squeeze(-1), y.float())\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        losses.append(total_loss)\n",
    "\n",
    "def eval_nn(model, test_dataset):\n",
    "    \"\"\"\n",
    "    Evaluates binary classification performance of a model on the given\n",
    "    test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    loss_fun = nn.BCELoss()\n",
    "    predictions = model(test_dataset.samples.to(DEVICE))\n",
    "    labels = test_dataset.labels.unsqueeze(-1).to(DEVICE)\n",
    "    loss = loss_fun(predictions, labels.float()).item()\n",
    "\n",
    "    predictions = predictions.squeeze(-1).detach()\n",
    "    labels = labels.squeeze(-1).detach()\n",
    "    f1 = binary_f1_score(predictions, labels).item()\n",
    "    accuracy = binary_accuracy(predictions, labels).item()\n",
    "    auc = binary_auroc(predictions, labels).item()\n",
    "\n",
    "    return loss, f1, accuracy, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3495eeb-14fa-4ae2-aaaf-b24c64ab8ef9",
   "metadata": {},
   "source": [
    "In this cell, we initialise and train the neural model that we will be explaining in this coursework. For a real-world application, you would typically wish to perform a full hyperparameter search in order to identify the most effective model architecture. However, achieving a maximum performance is not the objective of this coursework, so we just pre-define a model that performs reasonably well on the given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68254f96-7821-4a53-8615-2edbfee294ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def print_metric(name, value):\n",
    "    print(f\"{name}: {'{:.2f}'.format(round(value, 2))}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "model = construct_nn([7, 256, 256, 1], nn.ReLU).to(DEVICE)\n",
    "\n",
    "print(\"———————[ Model training ]———————\")\n",
    "train_nn(model, train_dl, num_epochs=1000)\n",
    "print(\"Training completed!\")\n",
    "print()\n",
    "\n",
    "print(\"———————[ Evaluation ]———————\")\n",
    "test_loss, f1, accuracy, auc = eval_nn(model, test_dataset)\n",
    "print_metric(\"F1 score\", f1)\n",
    "print_metric(\"Accuracy\", accuracy)\n",
    "print_metric(\"AUC\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6034c1b-500d-4e02-a6e7-742d62208c8e",
   "metadata": {},
   "source": [
    "## Feature Attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fa2e83-1342-4a8c-b1e7-d1188455eeb6",
   "metadata": {},
   "source": [
    "In this section of the coursework, you will implement SHAP as introduced in the lectures and conduct additional experiments with various feature attribution methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e7328-82bd-41aa-bcb9-6488e575c3f4",
   "metadata": {},
   "source": [
    "### SHAP Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf73aa-f5e3-4c36-9875-19e38c5d5c99",
   "metadata": {},
   "source": [
    "**Task 2(a)(i)**: As a first step in implementing SHAP, define a `compute_coefficient` function to compute the SHAP coalition coefficient/weight as specified by the formula from the lectures:\n",
    "\n",
    "$$g_{SHAP}(\\mathcal{M},\\mathbf{x},i) = \\sum_{\\mathbf{z} \\subseteq \\mathbf{x}}{\\frac{|\\mathbf{z}|!(n - |\\mathbf{z}| - 1)! }{n!} \\mathcal{M}( \\mathbf{z}) - \\mathcal{M}(\\mathbf{z}_{-i})} \\nonumber$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564deb60-0275-4b75-a2a8-b2e61ef4563c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def compute_coefficient(num_in_coalition, total_features):\n",
    "    \"\"\"\n",
    "    Computes the SHAP coefficient for a coalition.\n",
    "\n",
    "    Parameters:\n",
    "        num_in_coalition (int): The number of features in the given coalition\n",
    "        total_features (int): The total number of considered features\n",
    "\n",
    "    Returns:\n",
    "        coefficient (float): The SHAP weight for the given coalition\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8131b77-5eac-471c-a1a8-ff138f4b4541",
   "metadata": {},
   "source": [
    "**Task 2(a)(ii)**: Next, define a function `generate_coalitions`, which will return the list representing all the possible coalitions for a possible feature.\n",
    "\n",
    "Hint #1: You may find it helpful to use [itertools](https://docs.python.org/3/library/itertools.html) and [Python generators](https://wiki.python.org/moin/Generators) for implementing this function.\n",
    "\n",
    "Hint #2: Passing a full list of feature IDs is not strictly necessary here, but you will find this list helpful for implementing other functions, so we also recommend taking it as a parameter here. As an example, for the Titanic dataset, this list could look like `[0, 1, 2, 3, 4, 5, 5]` (note the repeated `5` for the one-hot-encoded `sex` feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e60209-d01b-4947-bba5-e13b5385d43f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_coalitions(feature_ids, target_feature_id):\n",
    "    \"\"\"\n",
    "    Generates the possible feature coalitions for the purpose of computing the Shapley value\n",
    "    for the target feature.\n",
    "\n",
    "    Parameters:\n",
    "        feature_ids (list): A list with feature IDs from 0 to N (where N is\n",
    "            the total number of features) identifying the used features. Distinct\n",
    "            columns for one-hot-encoded features should be assigned the same\n",
    "            numerical ID.\n",
    "        target_feature_id (int): The ID of the removed feature for which the coalitions\n",
    "            should be generated.\n",
    "\n",
    "    Retruns:\n",
    "        coalitions (list): A nested list structure of coalitions in the form:\n",
    "            [(set(coalition 1 in features set), set(coalition 1 out features set)), ...].\n",
    "            Note that feature_id should not appear in either of the in/out lists.\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6fe694-5010-47c6-a3dc-68ffd032eb1b",
   "metadata": {},
   "source": [
    "**Task 2(a)(iii)**: Next, implement a function `delete_features` that deletes the specified features from the given input tensor `x`. In contrast with the setting in the SHAP tutorial, the majority of features considered in this coursework are non-binary, which makes the deletion of features slightly more challenging. The general procedure for performing the deletion can be described as follows:\n",
    "1. For each sample in `x` and each deleted feature, randomly sample the value of the deleted feature from another data point in the background dataset\n",
    "2. If the sampled value is identical to the current value of the deleted feature, continue sampling new values until finding one that differs. This ensures that feature deletion actually changes the values of categorical variables or variables with few possible values.\n",
    "3. Replace the value of the deleted feature in the currently considered sample with the newly sampled value\n",
    "\n",
    "Hint #1: Boolean tensor masks \"selecting\" certain features can be very helpful here.\n",
    "\n",
    "Hint #2: Make sure not to overwrite values in the original `x` when deleting features. Instead, the function should return a new tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75928ee-3f2b-4427-8f41-45cfbe239eda",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def delete_features(x, background_dataset, feature_ids, deleted_feature_ids):\n",
    "    \"\"\"\n",
    "    Deletes the specified features from inputs x using the background dataset.\n",
    "\n",
    "    Parameters:\n",
    "        x (Tensor): A tensor of inputs with the shape (batch_size, num_features).\n",
    "        background_dataset (Tensor): A tensor of background data samples with the same shape as x.\n",
    "        feature_ids (list): A list with feature IDs, same as in generate_coalitions.\n",
    "        deleted_feature_ids (set): A set with feature IDs to be deleted from x.\n",
    "\n",
    "    Returns:\n",
    "        x_deleted (tensor): A new tensor of inputs with the specified features deleted.\n",
    "    \"\"\"    \n",
    "    # Make the sampling deterministic\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # TODO: Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679f0dc8-1542-4989-b148-41d5e5889e5b",
   "metadata": {},
   "source": [
    "**Task 2(a)(iv)**: Finally, put everything together in the `shap_attribute` function, which will compute the SHAP attributions for the given input and model. Note that the function also takes in a `target_idx` specifying for which output neuron the explanations should be computed. This is not strictly necessary for the Titanic model, which only has a single Sigmoid output, but will be needed once you start working with a more complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecaa2ba-f48f-474e-9503-d22598e554e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def shap_attribute(model, x, background_dataset, feature_ids, target_idx=0):\n",
    "    \"\"\"\n",
    "    Computes the SHAP attributions for the given input and model.\n",
    "\n",
    "    Parameters:\n",
    "        model (Object): A PyTorch model for which the attributions should be computed.\n",
    "        x (Tensor): Inputs for which the explanations should be computed, in shape (batch_size, num_features).\n",
    "        background_dataset (Tensor): A tensor of background data samples with the same shape as x.\n",
    "        feature_ids (list): A list with feature IDs, same as in generate_coalitions.\n",
    "        target_idx (int): The ID of the target neuron for which to compute an explanation\n",
    "            (useful for classification tasks with multiple labels where it should correspond\n",
    "\n",
    "    Returns:\n",
    "        attributions (Tensor): A tensor of SHAP attributions, with the same shape as the input\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3017834b-bf1c-48e4-ab1c-ff0d26ccab5c",
   "metadata": {},
   "source": [
    "### Additional Explanation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33325644-8c95-4e9f-aa94-9cf9206d2268",
   "metadata": {},
   "source": [
    "Apart from SHAP, which you just implemented, you will also be experimenting with two more feature attribution methods implemented in the [Captum](https://captum.ai/) library — [Shapley Value Sampling](https://captum.ai/api/shapley_value_sampling.html) and [DeepLIFT](https://captum.ai/api/deep_lift.html). Shapley Value Sampling is a more computationally tractable approximation of SHAP and computes the scores by randomly sampling a fixed number of coalitions instead of considering all of them. Meanwhile, DeepLIFT is a fast gradient-based attribution method specifically designed for neural models. If you are interested, you can learn more about DeepLIFT in [its original paper](https://arxiv.org/abs/1704.02685).\n",
    "\n",
    "To get you started, we provide an example of how to use the Captum library to generate Shapley Value Sampling attributions for the first sample from the Titanic test set (note that the library also allows you to compute attributions for a batch of inputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974eca85-1658-4222-95fc-bf3d180baeb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from captum.attr import ShapleyValueSampling\n",
    "\n",
    "# Note that this is similar to feature_ids from the implementation above,\n",
    "# but the shape and the data type are different\n",
    "feature_mask = torch.tensor([[0, 1, 2, 3, 4, 5, 5]]).to(DEVICE)\n",
    "svs = ShapleyValueSampling(model)\n",
    "attributions = svs.attribute(test_dataset.samples[[0]].to(DEVICE), target=0, feature_mask=feature_mask)\n",
    "attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e108b2-15d8-4c0b-bf6f-a64a4fe3b9e2",
   "metadata": {},
   "source": [
    "Notice that, in contrast with the SHAP implementation above, we did not need to pass in the background dataset. This is because Captum takes a slightly different approach to deleting features and instead replaces them with a pre-specified baseline value (see the `baselines` parameter description in the [documentation](https://captum.ai/api/shapley_value_sampling.html)). For the purposes of this coursework, it is fine to use the default (zero) baseline for both Shapley Value Sampling and DeepLIFT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c24e08-7871-44fe-afdd-d15c3c8d1fee",
   "metadata": {},
   "source": [
    "### Feature Attribution Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d4669-00e6-43bb-826f-c0f12aa45f96",
   "metadata": {},
   "source": [
    "In this section, you will conduct several experiments associated with feature attribution methods.\n",
    "\n",
    "**Task 2(b)**: Using your implementation of SHAP and Captum implementations of Shapley Value Sampling and DeepLIFT, compute feature attributions for 10 randomly selected instances from the Titanic test set. Then answer the following questions: <br />\n",
    "**(i)** Which features generally seem to be the most important and least important for the explained model according to each of the explanations? <br />\n",
    "**(ii)** Are there any substantial differences between the different attribution methods? What might be the possible reasons for the different methods returning different attribution scores? <br />\n",
    "**(iii)** Do the attribution scores match your expectations for the most/least important features from task 1(a)(ii)? What might be the reasons for a user's expected explanations differing from the computed attribution explanations? <br />\n",
    "**(iv)** Considering the insights gained from the exploratory data analysis and the feature attribution explanations, as well as the definitions of the explanations themselves, what are the potential advantages/disadvantages of each of these methods when trying to understand the behaviour of a model on a particular dataset? <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2680c1c-9276-45c1-94ae-c10a660b74f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# TODO: Your code and experiments here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f1001-218e-46a8-b4cf-303f2939c234",
   "metadata": {},
   "source": [
    "**Task 2(c)**: Perform a quantitative evaluation of the different attribution methods by computing their mean [infidelity](https://captum.ai/api/metrics.html) on the full Titanic dataset. On a high level, infidelity aims to estimate how closely the generated explanations correspond with the behaviour of the explained model by slightly perturbing the inputs and measuring how much the observed change in the model output differs from the change predicted by the corresponding feature attributions (when considering a linear model with the same weights as the feature attribution scores). If you are interested, you can find more details regarding this metric in [the original paper](https://arxiv.org/abs/1901.09392). A downside of the infidelity metric is that one needs to define a suitable perturbation function for changing the model inputs, which can significantly affect the results. In this coursework, we provide you with a perturbation function adding Gaussian noise to continuous features and performing resampling for categorical features. In your evaluation, you should experiment with two or three different standard deviations and categorical resampling probabilities. Once you are done, add a table summarising the results to your report and comment on the findings. Note that lower infidelity scores are better.\n",
    "\n",
    "Note: You should use `normalize=True` and `n_perturb_samples=10` as parameters to the Captum's infidelity function and set the same Torch and NumPy seeds before computing the infidelity for each method (so that all methods are evaluated using the same sample perturbations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffeb0f4-180f-4c7c-8a3f-258d3725ddfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from captum.metrics import infidelity, infidelity_perturb_func_decorator\n",
    "\n",
    "def perturb_func_constructor(noise_scale, cat_resample_proba, background_dataset, feature_ids, n_perturb_samples=10):\n",
    "    \"\"\"\n",
    "    You can call this function to construct a perturbation function with the desired parameters,\n",
    "    which can then be provided as the perturb_func parameter to the infidelity metric implementation\n",
    "    from Captum.\n",
    "\n",
    "     Parameters:\n",
    "        noise_scale (float): A standard deviation of the Gaussian noise added to the continuous features.\n",
    "        cat_resample_proba (float): Probability of resampling a categorical feature.\n",
    "        background_dataset (Tensor): A tensor of background data samples with the shape (num_samples, num_features).\n",
    "        feature_ids (list): A list with feature IDs, same as in generate_coalitions.\n",
    "        n_perturb_samples (int): The number of perturbed samples for each input. Should match the value\n",
    "            of the corresponding parameter to the Captum's infidelity function.\n",
    "\n",
    "    Returns:\n",
    "        perturb_func (function): A perturbation function compatible with Captum\n",
    "    \"\"\"\n",
    "    @infidelity_perturb_func_decorator(True)\n",
    "    def perturb_func(inputs):        \n",
    "        # Construct masks for noise and resampling categorical variables\n",
    "        noise_mask = torch.ones(1, inputs.size(1)).to(DEVICE)\n",
    "        # We assume that categorical features are one-hot-encoded\n",
    "        i = 0\n",
    "        current_span_start = 0\n",
    "        categorical_spans = []\n",
    "        while i < len(feature_ids) - 1:    \n",
    "            if feature_ids[i] != feature_ids[i + 1] and current_span_start != i:\n",
    "                categorical_spans.append((current_span_start, i))\n",
    "                current_span_start = i + 1\n",
    "            elif feature_ids[i] != feature_ids[i + 1]:\n",
    "                current_span_start = i + 1\n",
    "            elif feature_ids[i] == feature_ids[i + 1] and i == len(feature_ids) - 2:\n",
    "                categorical_spans.append((current_span_start, i + 1))\n",
    "            i += 1\n",
    "                \n",
    "        cat_resample_masks = []\n",
    "        for i, (s, e) in enumerate(categorical_spans):\n",
    "            cat_resample_mask = torch.zeros(inputs.shape).to(DEVICE)\n",
    "            probabilities = torch.full((inputs.size(0), 1), cat_resample_proba)\n",
    "            resample_tensor = torch.bernoulli(probabilities)\n",
    "            noise_mask[:, s:e] = 0.\n",
    "            cat_resample_mask[:, s:e] = resample_tensor\n",
    "            cat_resample_masks.append(cat_resample_mask)\n",
    "\n",
    "        # Add noise to continuous features only\n",
    "        noise = torch.tensor(np.random.normal(0, noise_scale, inputs.shape)).float().to(DEVICE) * noise_mask\n",
    "        perturbed_inputs = inputs - noise\n",
    "\n",
    "        # Randomly resample categorical variables\n",
    "        if categorical_spans:\n",
    "            expanded_background_dataset = background_dataset.repeat((n_perturb_samples, 1))\n",
    "            for cat_resample_mask in cat_resample_masks:\n",
    "                random_perm = torch.randperm(expanded_background_dataset.size(0))\n",
    "                random_samples = expanded_background_dataset[random_perm[:inputs.size(0)]]\n",
    "                perturbed_inputs = perturbed_inputs * (1 - cat_resample_mask) + random_samples * cat_resample_mask\n",
    "\n",
    "        return perturbed_inputs\n",
    "\n",
    "    return perturb_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36cb06b-f3ae-4888-905f-54d8cb5ae7bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# TODO: Your code and experiments here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611be864-83ac-467c-8925-e5dfa6927463",
   "metadata": {},
   "source": [
    "**Task 2(d)**: Evaluate the computational efficiency of the different methods by taking the following steps: <br />\n",
    "**(i)** Preproccess the [Dry Bean Dataset](https://archive.ics.uci.edu/dataset/602/dry+bean+dataset), similarly to what we have done for Titanic. You can find the description of the different features on the dataset webpage along with the instructions on how to import the data in a Python environment. You do not need to perform any exploratory data analysis for this dataset. <br />\n",
    "**(ii)** Train an additional neural model on the preprocessed data. Briefly report the key performance metrics for the model in your report. <br />\n",
    "**(iii)** Compute the runtimes required to produce the attribution scores for the different methods when considering the first 200 samples in the Titanic and Dry Bean test sets. Report the results in a table in your report. Which methods seem to be the most/least computationally efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d411bc2f-3ece-4352-8e2f-66c771e5916e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# TODO: Your code and experiments here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ec783c-e350-473e-b3e1-140095ebed53",
   "metadata": {},
   "source": [
    "## Counterfactual Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb92884-22cd-46e4-824d-8354e35ce856",
   "metadata": {},
   "source": [
    "### Designing a Distance Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f2aac-ad49-4a47-a74a-21158879f06d",
   "metadata": {},
   "source": [
    "**Task 3(a)**: First, we need to specify a suitable distance metric for measuring the closeness between the points. Depending on the dataset, one could choose the standard distance functions like the Manhattan (L1) distance, Euclidean (L2) distance, and more specialised ones like Gower distance for better handling datasets with both categorical and continuous dataset. However, the design of distance metric can be very flexible. For example, the standard L1 distance is (k is the number of features) $$d_{L1}(x, x') = \\sum_{i}^{k} |x_i-x'_i|$$\n",
    "\n",
    "If the features have different value ranges, we could normalise the L1 distance with the maximum and minimum values of each feature (indexed $i$) in the training dataset, $max_i$, $min_i$ : $$d_{L1, normalised}(x, x') = \\sum_{i}^{k} |(x_i-x'_i)/(max_i-min_i)|$$\n",
    "\n",
    "On top of this, we could also add customised weighting factors $\\mathbf{w}=w_1, ..., w_k$ to capture the importance of each feature, and the weighted L1 distance is: $$d_{L1, normalised, weighted}(x, x') = \\sum_{i}^{k} w_i|(x_i-x'_i)/(max_i-min_i)|$$\n",
    "\n",
    "Given the background above, we want to design a distance function for the preprocessed version of our Titanic dataset. Explore the dataset characteristics and answer the following questions:\n",
    "**a)** Briefly discuss the weighting of each input variable in the preprocessed dataset, if we use standard L1 and normalised L1?\n",
    "**b)** If we want to treat each feature equally in the original unprocessed dataset, how would you design the distance metric for the preprocessed dataset using L1-based distance? Write down the detail of your distance function for the preprocessed dataset and justify why each original feature is treated equally.\n",
    "**c)** Implement your distance function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b077fa1-b082-438a-806b-119714ca33eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def distance_function(x1, x2):\n",
    "    \"\"\"\n",
    "    Your distance function.\n",
    "\n",
    "    Parameters:\n",
    "        x1 (Tensor): A 1-d array of shape (k,)\n",
    "        x2 (Tensor): A 1-d array of shape (k,)\n",
    "\n",
    "    Returns:\n",
    "        distance (float): A real number >= 0\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df345ad0-6bdb-47b9-abf9-598c23a963c4",
   "metadata": {},
   "source": [
    "### Nearest-Neighbour Counterfactual Explanations (NNCE)\n",
    "\n",
    "**Task 3(b)**: As introduced in the tutorial, NNCEs are a simple yet effective method for finding counterfactuals. Implement the NNCE functions.\n",
    "\n",
    "Instructions:\n",
    "1. determine desired label for the counterfactual\n",
    "2. find the dataset points with desired label as predicted by the model\n",
    "3. find the point with the minimum distance and return it as NNCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47409a30-fbad-4371-99d6-b87d30cd6590",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def compute_nnce(x, m, train_set, dist):\n",
    "    \"\"\"\n",
    "    Function to compute NNCE.\n",
    "\n",
    "    Parameters:\n",
    "        x (Tensor): Input, a 1-d array of shape (k,)\n",
    "        m (Sequential): Our neural network\n",
    "        train_set (TitanicDataset): Our Titanic dataset\n",
    "        dist (function): Your previously implemented distance function\n",
    "\n",
    "    Returns:\n",
    "        nnce (Tensor): Nearest neighbour counterfactual explanation, an 1-d array of shape (k,)\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e25d4e-eb20-4a62-a468-64ce6834d6ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# use this code block to test if your function is working\n",
    "# first print out the original input's prediction result\n",
    "test_input = test_dataset.samples.to(DEVICE)[0]\n",
    "print(model(test_input))\n",
    "\n",
    "# now compute NNCE and print out the NNCE's prediction result. Ideally this is different from the result for the original input.\n",
    "nnce = compute_nnce(test_input, model, train_set=train_dataset, dist=distance_function)\n",
    "print(model(nnce))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6290a-7c2b-4064-8ed0-3a4056bac9dc",
   "metadata": {},
   "source": [
    "### Gradient-Based Counterfactual Explanations\n",
    "\n",
    "**Task 3(c)**: Complete the PyTorch implementation for the gradient-based method in [Wachter et al. 2017]: WAC.\n",
    "\n",
    "Instructions:\n",
    "1. We are going to optimise the following loss function to find a counterfactual x': $  argmin_{x'} \\text{ } BCE(y', (1-y)) + \\lambda cost(x, x')$, where $BCE$ is binary cross entropy loss, $y'$ is the predicted label of $x'$, $y$ is the predicted label of $x$, $cost(,)$ is your chosen distance function in Task 1, and $\\lambda$ is the trade-off parameter between validity and proximity. First, implement your chosen distance metric in ```CostLoss.forward()```\n",
    "2. Follow the code structure in the ```compute_wac()``` function, complete the implementation.\n",
    "    2.1. specify the target label for the counterfactual\n",
    "    2.2. implement gradient descent procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6aae81-d6bb-4037-810d-198c00143334",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CostLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(CostLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        The PyTorch version of your distance function\n",
    "\n",
    "        Parameters:\n",
    "            x1 (Tensor): A 1-d array of shape (k,)\n",
    "            x2 (Tensor): A 1-d array of shape (k,)\n",
    "\n",
    "        Returns:\n",
    "            distance (Tensor): a real number\n",
    "        \"\"\"\n",
    "        # TODO: Here is an example of standard L1 loss, replace it with your designed distance function in Task 1\n",
    "        dist = torch.abs(x1 - x2)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ae1f7-8c47-4a71-a817-2f22d48a64b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "import datetime\n",
    "\n",
    "def compute_wac(x, m, lamb=0.1, lr=0.01, max_iter=1000, max_allowed_minutes=0.5):\n",
    "    \"\"\"\n",
    "    Function to find WAC using gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "        x (Tensor): Input x, an 1-d array of shape (k,)\n",
    "        m (Sequential): PyTorch model\n",
    "        lamb (float): Lambda, the tradeoff term in the loss function\n",
    "        lr (float): Learning rate for gradient descent\n",
    "        max_iter (int): maximum allowed iteration\n",
    "        max_allowed_minutes (float): maximum allowed minutes\n",
    "\n",
    "    Returns:\n",
    "        counterfactual (Tensor): Counterfactual point, an 1-d array of shape (k,)\n",
    "    \"\"\"\n",
    "    # initialise the counterfactual search at the input point\n",
    "    x = x.to(DEVICE)\n",
    "    wac = Variable(x.clone(), requires_grad=True).to(DEVICE)\n",
    "\n",
    "    # initialise an optimiser for gradient descent over the wac counterfactual point\n",
    "    optimiser = Adam([wac], lr, amsgrad=True)\n",
    "\n",
    "    # instantiate the two components of the loss function\n",
    "    validity_loss = torch.nn.BCELoss()\n",
    "    cost_loss = CostLoss()\n",
    "\n",
    "    # TASK: specify target label y: either 0 or 1, depending on the original prediction\n",
    "    # TODO: Start your code here\n",
    "\n",
    "    # this line below is a placeholder, change this\n",
    "    y_target = torch.Tensor([1]).to(DEVICE)\n",
    "\n",
    "    # the total loss in the instructions: loss = validity_loss + lamb * cost_loss\n",
    "    # TODO: End your code here\n",
    "\n",
    "    # compute class probability\n",
    "    class_prob = m(wac)\n",
    "    wac_valid = False\n",
    "    iterations = 0\n",
    "    if y_target == 0 and class_prob < 0.5 or y_target == 1 and class_prob >= 0.5:\n",
    "        wac_valid = True\n",
    "\n",
    "    # set maximum allowed time for computing 1 counterfactual\n",
    "    t0 = datetime.datetime.now()\n",
    "    t_max = datetime.timedelta(minutes=max_allowed_minutes)\n",
    "    # start gradient descent\n",
    "    while not wac_valid or iterations <= max_iter:\n",
    "\n",
    "        # TASK: gradient descent to find wac\n",
    "        # TODO: Your code here\n",
    "\n",
    "        # break conditions: valid counterfactual found, or iterations exceeded, or reached allowed max time\n",
    "        if y_target == 0 and class_prob < 0.5 or y_target == 1 and class_prob >= 0.5:\n",
    "            wac_valid = True\n",
    "        if datetime.datetime.now() - t0 > t_max:\n",
    "            break\n",
    "        iterations += 1\n",
    "\n",
    "    return wac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3730ddc2-6dbe-46a2-8bf3-e833800c52ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# use this code block to test if your function is working\n",
    "# first print out the original input's prediction result\n",
    "test_input = test_dataset.samples.to(DEVICE)[1]\n",
    "print(model(test_input))\n",
    "\n",
    "# now compute WAC and print out the WAC's prediction result. Ideally this is different from the result for the original input.\n",
    "nnce = compute_wac(test_input, model)\n",
    "print(model(nnce))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb7d395-b999-4100-85dd-7b5eed5c57d7",
   "metadata": {},
   "source": [
    "### Performance of the Two Methods\n",
    "\n",
    "**Task 3(d)**: In order to understand better how these two methods compare, we use the following metrics to quantitatively evaluate each of the methods:\n",
    "- Validity: percentage of the counterfactuals that are valid.\n",
    "- Proximity: average distance between the counterfactuals and the inputs. Smaller distance (lower cost) indicates better proximity.\n",
    "- Plausibility: average distance of a counterfactual to its 5 nearest neighbours in the training dataset, further averaged over all counterfactuals. The closer it is to the nearest neighbours, the more plausible. Consider this metric as a simplified version of Local Outlier Factor.\n",
    "\n",
    "For each counterfactual method, we randomly select 20 test inputs, generate counterfactuals for them, and compare the average performances for each of the metrics.\n",
    "We repeat this process for 5 times and calculate the mean and standard deviation of each metrics. We have provided code for these experiments. Complete the following codes for calculating the evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4847e8d0-c76c-462d-a986-ccd2dc300e49",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def calculate_three_metrics_for_group_of_inputs(inputs, m, counterfactuals, train_set, dist):\n",
    "    validity, proximity, plausibility = 0, 0, 0\n",
    "    # examine validity, proximity, plausibility for each input-counterfactual pair\n",
    "    for i, x in enumerate(inputs):\n",
    "        ce = counterfactuals[i]\n",
    "\n",
    "        this_val = calculate_validity(x, ce, m)\n",
    "        this_prox = calculate_proximity(x, ce, dist)\n",
    "        this_plaus = calculate_plausibility(ce, train_set, dist)\n",
    "\n",
    "        validity += this_val\n",
    "        proximity += this_prox\n",
    "        plausibility += this_plaus\n",
    "\n",
    "    # average evaluation metrics over all the test inputs\n",
    "    validity = validity / len(inputs)\n",
    "    proximity = proximity / len(inputs)\n",
    "    plausibility = plausibility / len(inputs)\n",
    "    return validity, proximity, plausibility\n",
    "\n",
    "\n",
    "# TASK: for each input-counterfactual pair, calculate validity, proximity, and plausibility\n",
    "\n",
    "# check whether a counterfactual ce is valid or not\n",
    "def calculate_validity(x, ce, m):\n",
    "    # TODO: Your code here\n",
    "    pass\n",
    "\n",
    "# check the distance between a counterfactual and the input\n",
    "def calculate_proximity(x, ce, dist):\n",
    "    # TODO: Your code here\n",
    "    pass\n",
    "\n",
    "# calculate plausibility of a counterfactual\n",
    "def calculate_plausibility(ce, train_set, dist):\n",
    "    # here, calculate the average distance between the counterfactual and its 5 nearest neighbours in the training dataset\n",
    "    \n",
    "    # TODO: Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1cb9d3-cb0b-4473-b7d2-166e76032c61",
   "metadata": {},
   "source": [
    "Now we set up the experiments, compute counterfactuals using NNCE and WAC, then evaluate and compare their performances. Note that depending on your machine, the computation for ```compute_wac()``` could potentially be slow. You can also manually change the function's hyperparameters ```lamb=0.1, lr=0.01, max_iter=1000``` to try and see if WAC could give better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10f578-a107-4fc5-8a33-246018d9b6c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# we store the evaluation results of the five runs in lists\n",
    "nnce_validity, nnce_proximity, nnce_plausibility = [], [], []\n",
    "wac_validity, wac_proximity, wac_plausibility = [], [], []\n",
    "seed_num = 1000\n",
    "\n",
    "# repeat over 5 runs to obtain more robust evaluations\n",
    "for one_run in tqdm(range(5)):\n",
    "    # randomly select 20 test inputs\n",
    "    np.random.seed(seed_num)\n",
    "    test_inputs = test_dataset.samples[np.random.choice(range(len(test_dataset.samples)), 20)]\n",
    "    nnce_counterfactuals, wac_counterfactuals = [], []\n",
    "    # generate counterfactuals\n",
    "    with tqdm(total=1, position=0, leave=True) as pbar:\n",
    "        for x in tqdm(test_inputs, position=0, leave=True):\n",
    "            nnce_counterfactuals.append(compute_nnce(x, model, train_dataset, distance_function))\n",
    "            wac_counterfactuals.append(compute_wac(x, model, lamb=0.1))\n",
    "            pbar.update()\n",
    "\n",
    "    # evaluate counterfactuals\n",
    "    nnvalidity, nnproximity, nnplausibility = calculate_three_metrics_for_group_of_inputs(test_inputs, model,\n",
    "                                                                                          nnce_counterfactuals,\n",
    "                                                                                          train_dataset,\n",
    "                                                                                          distance_function)\n",
    "    nnce_validity.append(nnvalidity)\n",
    "    nnce_proximity.append(nnproximity)\n",
    "    nnce_plausibility.append(nnplausibility)\n",
    "\n",
    "    wvalidity, wproximity, wplausibility = calculate_three_metrics_for_group_of_inputs(test_inputs, model,\n",
    "                                                                                       wac_counterfactuals,\n",
    "                                                                                       train_dataset, distance_function)\n",
    "    wac_validity.append(wvalidity)\n",
    "    wac_proximity.append(wproximity)\n",
    "    wac_plausibility.append(wplausibility)\n",
    "\n",
    "    seed_num += 10\n",
    "\n",
    "# now print out the results\n",
    "from tabulate import tabulate\n",
    "\n",
    "score_names = [\"method\", \"validity\", \"cost\", \"plausibility\"]\n",
    "score_table = [score_names,\n",
    "               [\"NNCE\", f\"{np.mean(nnce_validity).round(3)} +- {np.std(nnce_validity).round(3)}\",\n",
    "                f\"{np.mean(nnce_proximity).round(3)} +- {np.std(nnce_proximity).round(3)}\",\n",
    "                f\"{np.mean(nnce_plausibility).round(3)} +- {np.std(nnce_plausibility).round(3)}\"],\n",
    "               [\"WAC\", f\"{np.mean(wac_validity).round(3)} +- {np.std(wac_validity).round(3)}\",\n",
    "                f\"{np.mean(wac_proximity).round(3)} +- {np.std(wac_proximity).round(3)}\",\n",
    "                f\"{np.mean(wac_plausibility).round(3)} +- {np.std(wac_plausibility).round(3)}\"]]\n",
    "print(tabulate(score_table, headers='firstrow', tablefmt='outline'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2474358c-0c66-444a-b528-d4e99f1d4e78",
   "metadata": {},
   "source": [
    "### Performance Differences\n",
    "\n",
    "**Task 3(e)**: Discuss in your report commenting their performance based on the metrics. Link the findings to their theories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
